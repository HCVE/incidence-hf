{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: sklearn.tree._splitter.Splitter size changed, may indicate binary incompatibility. Expected 360 from C header, got 1160 from PyObject\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "\n",
    "from hcve_lib.custom_types import ExceptionValue\n",
    "from hcve_lib.utils import notebook_init\n",
    "\n",
    "notebook_init()\n",
    "from hcve_lib.evaluation_functions import average_group_scores\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "from mlflow import set_tracking_uri\n",
    "import pandas\n",
    "from deps.common import get_data_cached\n",
    "\n",
    "set_tracking_uri('http://localhost:5000')\n",
    "\n",
    "from plotly import express as px\n",
    "\n",
    "from hcve_lib.tracking import load_group_results\n",
    "%autoreload 2\n",
    "\n",
    "GROUPS = {\n",
    "    # 'stacking': '5e37865c-7821-4726-a015-56447f4b172d',\n",
    "    'coxnet': '89be5f83-5679-42f2-ae11-f0788f8353d5',\n",
    "    'gb': '52e729f7-0ae3-40a4-b639-8f9f3eaa3a36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]7.2s, 0.1min    : Loading get_data...\n",
      "____________________________________________get_data cache loaded - 1.3s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "data, metadata, X, y = get_data_cached()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hcve_lib.utils import get_y_split\n",
    "from deps.constants import RANDOM_STATE\n",
    "from statistics import mean\n",
    "from hcve_lib.functional import lagged\n",
    "\n",
    "averaged_results = {}\n",
    "\n",
    "for group_name, group_id in GROUPS.items():\n",
    "    group = load_group_results(group_id, load_models=True)\n",
    "    averaged_results[group_name] = average_group_scores(group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDFCoxnetSurvivalAnalysisT does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, prediction \u001B[38;5;129;01min\u001B[39;00m result\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     14\u001B[0m     _, y_test \u001B[38;5;241m=\u001B[39m get_y_split(y, prediction)\n\u001B[0;32m---> 15\u001B[0m     new_prediction \u001B[38;5;241m=\u001B[39m \u001B[43mprediction\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmethod\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprediction\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msplit\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprediction\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRANDOM_STATE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprediction\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmethod\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtime\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_test\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtte\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     y_proba \u001B[38;5;241m=\u001B[39m new_prediction[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_proba\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtte\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     25\u001B[0m     y_proba_filtered \u001B[38;5;241m=\u001B[39m y_proba[y_proba\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m i: \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(i, ExceptionValue))]\n",
      "File \u001B[0;32m~/projects/hcve_lib/hcve_lib/evaluation_functions.py:394\u001B[0m, in \u001B[0;36mpredict_survival\u001B[0;34m(X, y, split, model, random_state, method, time)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_survival\u001B[39m(\n\u001B[1;32m    375\u001B[0m     X: DataFrame,\n\u001B[1;32m    376\u001B[0m     y: Target,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    381\u001B[0m     time: Union[\u001B[38;5;28mint\u001B[39m, Iterable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m365\u001B[39m,\n\u001B[1;32m    382\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Prediction:\n\u001B[1;32m    383\u001B[0m     X_test \u001B[38;5;241m=\u001B[39m loc(\n\u001B[1;32m    384\u001B[0m         split[\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    385\u001B[0m         X,\n\u001B[1;32m    386\u001B[0m         ignore_not_present\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    387\u001B[0m     )\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Prediction(\n\u001B[1;32m    390\u001B[0m         split\u001B[38;5;241m=\u001B[39msplit,\n\u001B[1;32m    391\u001B[0m         X_columns\u001B[38;5;241m=\u001B[39mX\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mtolist(),\n\u001B[1;32m    392\u001B[0m         y_column\u001B[38;5;241m=\u001B[39my[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    393\u001B[0m         y_score\u001B[38;5;241m=\u001B[39mSeries(\n\u001B[0;32m--> 394\u001B[0m             \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    395\u001B[0m             index\u001B[38;5;241m=\u001B[39mX_test\u001B[38;5;241m.\u001B[39mindex,\n\u001B[1;32m    396\u001B[0m         ),\n\u001B[1;32m    397\u001B[0m         y_proba\u001B[38;5;241m=\u001B[39m{(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtte\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(time, Iterable) \u001B[38;5;28;01melse\u001B[39;00m time): predict_survival_proba(time, X_test, model)},\n\u001B[1;32m    398\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    399\u001B[0m         random_state\u001B[38;5;241m=\u001B[39mrandom_state,\n\u001B[1;32m    400\u001B[0m         method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m    401\u001B[0m     )\n",
      "File \u001B[0;32m~/projects/hcve_lib/hcve_lib/cv.py:323\u001B[0m, in \u001B[0;36mOptimizeEstimator.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m--> 323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_best_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/homage-fl-Ms4N7PDp/lib/python3.9/site-packages/sklearn/pipeline.py:458\u001B[0m, in \u001B[0;36mPipeline.predict\u001B[0;34m(self, X, **predict_params)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, name, transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter(with_final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    457\u001B[0m     Xt \u001B[38;5;241m=\u001B[39m transform\u001B[38;5;241m.\u001B[39mtransform(Xt)\n\u001B[0;32m--> 458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpredict_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/hcve_lib/hcve_lib/wrapped_sklearn.py:46\u001B[0m, in \u001B[0;36mDFWrapped.predict\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series:\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_fit_features(X)\n\u001B[0;32m---> 46\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m y_pred\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/homage-fl-Ms4N7PDp/lib/python3.9/site-packages/sksurv/linear_model/coxnet.py:341\u001B[0m, in \u001B[0;36mCoxnetSurvivalAnalysis.predict\u001B[0;34m(self, X, alpha)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;124;03m\"\"\"The linear predictor of the model.\u001B[39;00m\n\u001B[1;32m    325\u001B[0m \n\u001B[1;32m    326\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;124;03m        The predicted decision function\u001B[39;00m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 341\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m     coef, offset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_coef(alpha)\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m numpy\u001B[38;5;241m.\u001B[39mdot(X, coef) \u001B[38;5;241m-\u001B[39m offset\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/homage-fl-Ms4N7PDp/lib/python3.9/site-packages/sklearn/base.py:577\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[0;32m--> 577\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    578\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/homage-fl-Ms4N7PDp/lib/python3.9/site-packages/sklearn/utils/validation.py:899\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    893\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    894\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    895\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[1;32m    896\u001B[0m         )\n\u001B[1;32m    898\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[0;32m--> 899\u001B[0m         \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    901\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[43m            \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    903\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    904\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    906\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    907\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/homage-fl-Ms4N7PDp/lib/python3.9/site-packages/sklearn/utils/validation.py:146\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    125\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m allow_nan\n\u001B[1;32m    126\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m estimator_name\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[1;32m    131\u001B[0m             \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[1;32m    132\u001B[0m             msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    133\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    134\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    144\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    145\u001B[0m             )\n\u001B[0;32m--> 146\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n",
      "\u001B[0;31mValueError\u001B[0m: Input X contains NaN.\nDFCoxnetSurvivalAnalysisT does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "\n",
    "from deps.logger import logger\n",
    "\n",
    "for method, result in averaged_results.items():\n",
    "    calibration_x = []\n",
    "    calibration_y = []\n",
    "    calibration_cohort = []\n",
    "    calibration_x_label = []\n",
    "\n",
    "    y_proba_per_split = []\n",
    "    y_test_per_split = []\n",
    "    for _, prediction in result.items():\n",
    "        _, y_test = get_y_split(y, prediction)\n",
    "        new_prediction = prediction['method'].predict(\n",
    "            X=X,\n",
    "            y=y,\n",
    "            split=prediction['split'],\n",
    "            model=prediction['model'],\n",
    "            random_state=RANDOM_STATE,\n",
    "            method=prediction['method'],\n",
    "            time=y_test['data']['tte'],\n",
    "        )\n",
    "        y_proba = new_prediction['y_proba']['tte']\n",
    "        y_proba_filtered = y_proba[y_proba.map(lambda i: not isinstance(i, ExceptionValue))]\n",
    "\n",
    "        deleted = len(y_proba)-len(y_proba_filtered)\n",
    "        if deleted > 0:\n",
    "            logger.warning(f'Some TTE out of training range; n deleted = {deleted}')\n",
    "\n",
    "        y_proba_per_split.append(y_proba_filtered)\n",
    "        y_test_per_split.append(y_test['data'])\n",
    "\n",
    "    y_proba_merged = pandas.concat(y_proba_per_split)\n",
    "    y_test_merged = pandas.concat(y_test_per_split)\n",
    "\n",
    "    quantiles = numpy.quantile(y_proba_merged, q=numpy.linspace(0, 1, 10))\n",
    "\n",
    "    for quantile_from, quantile_to in lagged(quantiles):\n",
    "        y_proba_subset = y_proba_merged[\n",
    "            ((y_proba_merged >= quantile_from) &\n",
    "             (y_proba_merged < quantile_to))\n",
    "        ]\n",
    "        y_subset = y['data'].loc[y_proba_subset.index]\n",
    "        calibration_x.append(mean([quantile_from, quantile_to]))\n",
    "        calibration_y.append(y_subset['label'].value_counts()[0] / len(y_subset['label']))\n",
    "        calibration_x_label.append(f'{quantile_from}-{quantile_to}')\n",
    "\n",
    "    fig = px.scatter(\n",
    "        x=calibration_x,\n",
    "        y=calibration_y,\n",
    "        # range_x=[0,1], range_y=[0,1]\n",
    "        width=600,\n",
    "        height=600,\n",
    "        title=method,\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deps.logger import logger\n",
    "\n",
    "for method, result in averaged_results.items():\n",
    "    y_proba_per_split = []\n",
    "    y_test_per_split = []\n",
    "    for _, prediction in result.items():\n",
    "        _, y_test = get_y_split(y, prediction)\n",
    "        new_prediction = prediction['method'].predict(\n",
    "            X=X,\n",
    "            y=y,\n",
    "            split=prediction['split'],\n",
    "            model=prediction['model'],\n",
    "            random_state=RANDOM_STATE,\n",
    "            method=prediction['method'],\n",
    "            time=y_test['data']['tte'],\n",
    "        )\n",
    "        y_proba = new_prediction['y_proba']['tte']\n",
    "        y_proba_filtered = y_proba[y_proba.map(lambda i: not isinstance(i, ExceptionValue))]\n",
    "\n",
    "        deleted = len(y_proba)-len(y_proba_filtered)\n",
    "        if deleted > 0:\n",
    "            logger.warning(f'Some TTE out of training range; n deleted = {deleted}')\n",
    "\n",
    "        y_proba_per_split.append(y_proba_filtered)\n",
    "        y_test_per_split.append(y_test['data'])\n",
    "\n",
    "    y_proba_merged = pandas.concat(y_proba_per_split)\n",
    "    y_test_merged = pandas.concat(y_test_per_split)\n",
    "\n",
    "    quantiles = numpy.quantile(y_proba_merged, q=numpy.linspace(0, 1, 20))\n",
    "\n",
    "    for quantile_from, quantile_to in lagged(quantiles):\n",
    "        y_proba_subset = y_proba_merged[\n",
    "            ((y_proba_merged >= quantile_from) &\n",
    "             (y_proba_merged < quantile_to))\n",
    "        ]\n",
    "        y_subset = y['data'].loc[y_proba_subset.index]\n",
    "        calibration_x.append(mean([quantile_from, quantile_to]))\n",
    "        calibration_y.append(y_subset['label'].value_counts()[0] / len(y_subset['label']))\n",
    "        calibration_x_label.append(f'{quantile_from}-{quantile_to}')\n",
    "\n",
    "    fig = px.scatter(\n",
    "        x=calibration_x,\n",
    "        y=calibration_y,\n",
    "        # range_x=[0,1], range_y=[0,1]\n",
    "        width=400,\n",
    "        height=400,\n",
    "        title=method,\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
